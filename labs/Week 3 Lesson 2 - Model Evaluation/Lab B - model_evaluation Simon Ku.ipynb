{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Null accuracy, handling missing values\n",
    "2. Confusion matrix, sensitivity, specificity, setting a threshold\n",
    "3. Handling categorical features, interpreting logistic regression coefficients\n",
    "4. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Null Accuracy, Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Lab from last lesson on Logistic Regression. We will be revisting the ideas introduced using the Titanic dataset again.\n",
    "\n",
    "For a description of the Titanic dataset see this Kaggle page: https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675635276532\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: read the data from titanic.csv into a DataFrame\n",
    "import pandas as pd\n",
    "titanic = pd.read_csv('titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# TASK 2: define Pclass/Parch as the features and Survived as the response\n",
    "feature_cols = ['Pclass', 'Parch']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "#X = ___\n",
    "#y = ___\n",
    "\n",
    "# TASK 3: split the data into training and testing sets\n",
    "# I have added test_size to control and improve the model performance\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75,random_state=1)\n",
    "\n",
    "# TASK 4: fit a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# TASK 5: make predictions on testing set and calculate accuracy\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Parch        Age\n",
       "PassengerId                          \n",
       "1                 3      0  22.000000\n",
       "2                 1      0  38.000000\n",
       "3                 3      0  26.000000\n",
       "4                 1      0  35.000000\n",
       "5                 3      0  35.000000\n",
       "6                 3      0  29.699118\n",
       "7                 1      0  54.000000\n",
       "8                 3      1   2.000000\n",
       "9                 3      2  27.000000\n",
       "10                2      0  14.000000\n",
       "11                3      1   4.000000\n",
       "12                1      0  58.000000\n",
       "13                3      0  20.000000\n",
       "14                3      5  39.000000\n",
       "15                3      0  14.000000\n",
       "16                2      0  55.000000\n",
       "17                3      1   2.000000\n",
       "18                2      0  29.699118\n",
       "19                3      0  31.000000\n",
       "20                3      0  29.699118\n",
       "21                2      0  35.000000\n",
       "22                2      0  34.000000\n",
       "23                3      0  15.000000\n",
       "24                1      0  28.000000\n",
       "25                3      1   8.000000\n",
       "26                3      5  38.000000\n",
       "27                3      0  29.699118\n",
       "28                1      2  19.000000\n",
       "29                3      0  29.699118\n",
       "30                3      0  29.699118\n",
       "...             ...    ...        ...\n",
       "862               2      0  21.000000\n",
       "863               1      0  48.000000\n",
       "864               3      2  29.699118\n",
       "865               2      0  24.000000\n",
       "866               2      0  42.000000\n",
       "867               2      0  27.000000\n",
       "868               1      0  31.000000\n",
       "869               3      0  29.699118\n",
       "870               3      1   4.000000\n",
       "871               3      0  26.000000\n",
       "872               1      1  47.000000\n",
       "873               1      0  33.000000\n",
       "874               3      0  47.000000\n",
       "875               2      0  28.000000\n",
       "876               3      0  15.000000\n",
       "877               3      0  20.000000\n",
       "878               3      0  19.000000\n",
       "879               3      0  29.699118\n",
       "880               1      1  56.000000\n",
       "881               2      1  25.000000\n",
       "882               3      0  33.000000\n",
       "883               3      0  22.000000\n",
       "884               2      0  28.000000\n",
       "885               3      0  25.000000\n",
       "886               3      5  39.000000\n",
       "887               2      0  27.000000\n",
       "888               1      0  19.000000\n",
       "889               3      2  29.699118\n",
       "890               1      0  26.000000\n",
       "891               3      0  32.000000\n",
       "\n",
       "[891 rows x 3 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3\n",
       "1         8\n",
       "2         5\n",
       "3         3\n",
       "4         0\n",
       "5         0\n",
       "6         2\n",
       "7         1\n",
       "8         1\n",
       "9         8\n",
       "10       12\n",
       "11       26\n",
       "12       29\n",
       "13       47\n",
       "14       35\n",
       "15       40\n",
       "16       41\n",
       "17       15\n",
       "18        9\n",
       "19        6\n",
       "20       11\n",
       "21        3\n",
       "22       11\n",
       "23       15\n",
       "24        4\n",
       "25        1\n",
       "26        1\n",
       "27        2\n",
       "28        2\n",
       "29        0\n",
       "         ..\n",
       "17349    12\n",
       "17350    16\n",
       "17351     9\n",
       "17352     5\n",
       "17353     6\n",
       "17354    10\n",
       "17355     4\n",
       "17356     6\n",
       "17357     3\n",
       "17358     0\n",
       "17359     0\n",
       "17360     0\n",
       "17361     0\n",
       "17362     2\n",
       "17363     9\n",
       "17364    13\n",
       "17365    33\n",
       "17366    43\n",
       "17367    52\n",
       "17368    38\n",
       "17369    62\n",
       "17370    69\n",
       "17371    30\n",
       "17372    14\n",
       "17373    10\n",
       "17374    11\n",
       "17375     8\n",
       "17376     7\n",
       "17377    13\n",
       "17378    12\n",
       "Name: casual, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null accuracy\n",
    "\n",
    "Null accuracy is the accuracy that could be achieved by always predicting the **most frequent class**. It is a baseline against which you may want to measure your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40358744394618834\n",
      "0.5964125560538116\n"
     ]
    }
   ],
   "source": [
    "# compute null accuracy manually\n",
    "print (y_test.mean())\n",
    "print (1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596412556054\n"
     ]
    }
   ],
   "source": [
    "# equivalent function in scikit-learn\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dumb = DummyClassifier(strategy='most_frequent')\n",
    "dumb.fit(X_train, y_train)\n",
    "y_dumb_class = dumb.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.497757847534\n"
     ]
    }
   ],
   "source": [
    "# equivalent function in scikit-learn\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dumb1 = DummyClassifier(strategy='stratified')\n",
    "dumb1.fit(X_train, y_train)\n",
    "y_dumb1_class = dumb1.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb1_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47533632287\n"
     ]
    }
   ],
   "source": [
    "# equivalent function in scikit-learn\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dumb2 = DummyClassifier(strategy='uniform')\n",
    "dumb2.fit(X_train, y_train)\n",
    "y_dumb2_class = dumb2.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb2_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57399103139\n"
     ]
    }
   ],
   "source": [
    "# equivalent function in scikit-learn\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "from sklearn.dummy import DummyClassifier\n",
    "prior=0.01\n",
    "dumb3 = DummyClassifier(strategy='prior')\n",
    "dumb3.fit(X_train, y_train)\n",
    "y_dumb3_class = dumb3.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb3_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n",
    "\n",
    "One possible strategy is to just **drop missing values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Name          0\n",
       "Sex           0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Ticket        0\n",
       "Fare          0\n",
       "Cabin       687\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 11)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 11)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with any missing values\n",
    "titanic.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 11)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where Age is missing\n",
    "titanic[titanic.Age.notnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a better strategy is to **impute missing values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill missing values for Age with the mean age\n",
    "titanic.Age.fillna(titanic.Age.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 11)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check Age missing is imputed\n",
    "titanic[titanic.Age.notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "# equivalent function in scikit-learn, supports mean/median/most_frequent\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean', axis=1)\n",
    "titanic['Age'] = imp.fit_transform(titanic.Age.reshape(1,-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Parch        Age\n",
       "PassengerId                          \n",
       "1                 3      0  22.000000\n",
       "2                 1      0  38.000000\n",
       "3                 3      0  26.000000\n",
       "4                 1      0  35.000000\n",
       "5                 3      0  35.000000\n",
       "6                 3      0  29.699118\n",
       "7                 1      0  54.000000\n",
       "8                 3      1   2.000000\n",
       "9                 3      2  27.000000\n",
       "10                2      0  14.000000\n",
       "11                3      1   4.000000\n",
       "12                1      0  58.000000\n",
       "13                3      0  20.000000\n",
       "14                3      5  39.000000\n",
       "15                3      0  14.000000\n",
       "16                2      0  55.000000\n",
       "17                3      1   2.000000\n",
       "18                2      0  29.699118\n",
       "19                3      0  31.000000\n",
       "20                3      0  29.699118\n",
       "21                2      0  35.000000\n",
       "22                2      0  34.000000\n",
       "23                3      0  15.000000\n",
       "24                1      0  28.000000\n",
       "25                3      1   8.000000\n",
       "26                3      5  38.000000\n",
       "27                3      0  29.699118\n",
       "28                1      2  19.000000\n",
       "29                3      0  29.699118\n",
       "30                3      0  29.699118\n",
       "...             ...    ...        ...\n",
       "862               2      0  21.000000\n",
       "863               1      0  48.000000\n",
       "864               3      2  29.699118\n",
       "865               2      0  24.000000\n",
       "866               2      0  42.000000\n",
       "867               2      0  27.000000\n",
       "868               1      0  31.000000\n",
       "869               3      0  29.699118\n",
       "870               3      1   4.000000\n",
       "871               3      0  26.000000\n",
       "872               1      1  47.000000\n",
       "873               1      0  33.000000\n",
       "874               3      0  47.000000\n",
       "875               2      0  28.000000\n",
       "876               3      0  15.000000\n",
       "877               3      0  20.000000\n",
       "878               3      0  19.000000\n",
       "879               3      0  29.699118\n",
       "880               1      1  56.000000\n",
       "881               2      1  25.000000\n",
       "882               3      0  33.000000\n",
       "883               3      0  22.000000\n",
       "884               2      0  28.000000\n",
       "885               3      0  25.000000\n",
       "886               3      5  39.000000\n",
       "887               2      0  27.000000\n",
       "888               1      0  19.000000\n",
       "889               3      2  29.699118\n",
       "890               1      0  26.000000\n",
       "891               3      0  32.000000\n",
       "\n",
       "[891 rows x 3 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include Age as a feature\n",
    "feature_cols = ['Pclass', 'Parch', 'Age']\n",
    "X = titanic[feature_cols]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67264573991\n"
     ]
    }
   ],
   "source": [
    "# include Age as a feature\n",
    "feature_cols = ['Pclass', 'Parch', 'Age']\n",
    "X = titanic[feature_cols]\n",
    "\n",
    "# TASK : split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = ...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "# TASK : fit a logistic regression model\n",
    "#logreg...\n",
    "logreg = logreg.fit(X_train, y_train)\n",
    "# TASK : make predictions on testing set and calculate accuracy\n",
    "#y_pred_class = ...\n",
    "#print ...\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107,  21],\n",
       "       [ 52,  43]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a plotting function that will plot a nice confusion matrix see: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load confusion_matrix_nice.py\n",
    "# from confusion_matrix_nice import plot_confusion_matrix\n",
    "%run confusion_matrix_nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[107  21]\n",
      " [ 52  43]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEmCAYAAADr3bIaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4LA0M8vAYWlmloHD0swsA4elmVkG/x9J2zI5TYp4/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xba7bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an attractive confusion matrix\n",
    "cnf_mat = metrics.confusion_matrix(y_test, y_pred_class, labels = titanic.Survived.unique())\n",
    "class_labels = titanic.Survived.unique()\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_mat, class_labels,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45263157894736844"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the sensitivity\n",
    "43 / float(52 + 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45263157894736844"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the sensitivity\n",
    "#43 / float(52 + 43)\n",
    "#0.45263157894736844\n",
    "\n",
    "# general\n",
    "sensitivity=cnf_mat[1,1]/float(cnf_mat[1,0]+cnf_mat[1,1])\n",
    "sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the specificity\n",
    "# 107 / float(107 + 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8359375"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general, suitable for any outcome of cnf mat\n",
    "specificity=cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67295597484276726"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percision=cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[1,0])\n",
    "percision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8359375"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall=cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1])\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74564459930313576"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1=2*percision*recall/float((percision+recall))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74564459930313576"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1=2*cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[1,0])*cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1])/(cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[1,0])+cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1]))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilities\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xbc98d30>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb58db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the predicted probabilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred_prob)\n",
    "plt.xlabel('Predicted probability of survival')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change the threshold for predicting survived to increase sensitivity\n",
    "import numpy as np\n",
    "y_pred_class = np.where(y_pred_prob > 0.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# equivalent function in scikit-learn\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize(y_pred_prob.reshape(1,-1), 0.25).T\n",
    "#y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57 71]\n",
      " [27 68]]\n"
     ]
    }
   ],
   "source": [
    "# new confusion matrix\n",
    "print (metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7157894736842105\n"
     ]
    }
   ],
   "source": [
    "# new sensitivity\n",
    "print (68 / float(27 + 68))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4453125\n"
     ]
    }
   ],
   "source": [
    "# new specificity\n",
    "print (57 / float(57 + 71))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: Cross Validation Example on Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro to cross validation with bike share data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "#from sklearn.cross_validation import KFold\n",
    "bikeshare = pd.read_csv('bikeshare.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables and set outcome (dependent) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pd.get_dummies(bikeshare.weathersit, prefix='weather')\n",
    "modeldata = bikeshare[['temp', 'hum']].join(weather[['weather_1', 'weather_2', 'weather_3']])\n",
    "y = bikeshare.casual \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross validation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     1\n",
       "17     0\n",
       "18     1\n",
       "19     0\n",
       "20     1\n",
       "21     0\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     0\n",
       "26     1\n",
       "27     0\n",
       "28     0\n",
       "29     1\n",
       "30     0\n",
       "      ..\n",
       "862    0\n",
       "863    1\n",
       "864    0\n",
       "865    0\n",
       "866    1\n",
       "867    1\n",
       "868    0\n",
       "869    0\n",
       "870    1\n",
       "871    0\n",
       "872    1\n",
       "873    0\n",
       "874    0\n",
       "875    1\n",
       "876    1\n",
       "877    0\n",
       "878    0\n",
       "879    0\n",
       "880    1\n",
       "881    1\n",
       "882    0\n",
       "883    0\n",
       "884    0\n",
       "885    0\n",
       "886    0\n",
       "887    0\n",
       "888    1\n",
       "889    0\n",
       "890    1\n",
       "891    0\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "Model 1\n",
      "MSE: 1471.3445964\n",
      "R2: 0.311905323577\n",
      "Model 2\n",
      "MSE: 1742.20506263\n",
      "R2: 0.311922281387\n",
      "Model 3\n",
      "MSE: 1687.75980234\n",
      "R2: 0.311899688573\n",
      "Model 4\n",
      "MSE: 1702.09378791\n",
      "R2: 0.311924765387\n",
      "Model 5\n",
      "MSE: 1762.2200335\n",
      "R2: 0.311898020196\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.12465656\n",
      "Mean of R2 for all folds: 0.311910015824\n"
     ]
    }
   ],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ Single Model ~~~~\n",
      "MSE of single model: 1672.58110765\n",
      "R2:  0.311934605989\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "print (\"~~~~ Single Model ~~~~\")\n",
    "print ('MSE of single model:', metrics.mean_squared_error(y, lm.predict(modeldata)))\n",
    "print ('R2: ', lm.score(modeldata, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "While the cross validated approach here generated more overall error, which of the two approaches would predict new data more accurately: the single model or the cross validated, averaged one? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "It eppears the single model with higher R-square and lower MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Apply Cross Validation to the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv('titanic.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "3                   1       3   \n",
       "4                   1       1   \n",
       "5                   0       3   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "5                                     Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                          \n",
       "1                1      0         A/5 21171   7.2500   NaN        S  \n",
       "2                1      0          PC 17599  71.2833   C85        C  \n",
       "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4                1      0            113803  53.1000  C123        S  \n",
       "5                0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean', axis=1)\n",
    "titanic['Age'] = imp.fit_transform(titanic.Age.reshape(1,-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0xc111388>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = ['Pclass', 'Parch', 'Age']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg=LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8         0.7         0.3         0.7         0.7         0.6         0.8\n",
      "  0.5         0.6         0.9         0.5         0.5         0.6         0.6\n",
      "  0.6         0.5         0.7         0.6         0.7         0.8         0.7\n",
      "  0.8         0.8         0.7         0.5         0.5         0.9         0.7\n",
      "  0.8         0.6         0.8         0.5         0.9         0.6         0.4\n",
      "  0.6         0.9         0.8         0.6         0.7         0.7         0.8\n",
      "  0.88888889  0.66666667  0.77777778  0.77777778  0.66666667  0.77777778\n",
      "  0.55555556  0.875       0.75        0.625       0.625       0.75        0.625\n",
      "  0.75        0.75        0.75        0.75        0.75        0.625       0.75\n",
      "  0.875       0.625       0.875       1.          0.875       0.75        0.875\n",
      "  0.75        0.5         0.75        0.75        0.625       0.875       0.625\n",
      "  0.75        0.875       0.875       0.875       0.625       0.875       0.625\n",
      "  0.625       0.875       0.75        0.625       0.5         0.75        0.625\n",
      "  0.75        0.625       0.75        0.75        1.          0.875       0.625\n",
      "  0.625       0.75        1.        ]\n",
      "0.712361111111\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=100)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Evaluate each stage of the model with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Evaluate the Cross-Validation score with different values of n (2, 5, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf2 = cross_validation.KFold(len(modeldata), n_folds=2, shuffle=True,random_state=1)\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.KFold(n=17379, n_folds=2, shuffle=True, random_state=1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "Model 1\n",
      "MSE: 1686.74885662\n",
      "R2: 0.311844116323\n",
      "Model 2\n",
      "MSE: 1659.68577407\n",
      "R2: 0.311850332503\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.21731534\n",
      "Mean of R2 for all folds: 0.311847224413\n"
     ]
    }
   ],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf2:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "Model 1\n",
      "MSE: 1860.10075325\n",
      "R2: 0.311924662002\n",
      "Model 2\n",
      "MSE: 1576.35815877\n",
      "R2: 0.311905863323\n",
      "Model 3\n",
      "MSE: 1611.93272006\n",
      "R2: 0.311912720153\n",
      "Model 4\n",
      "MSE: 1676.68644185\n",
      "R2: 0.3119285077\n",
      "Model 5\n",
      "MSE: 1709.00635897\n",
      "R2: 0.31192288973\n",
      "Model 6\n",
      "MSE: 1624.26000865\n",
      "R2: 0.311924285114\n",
      "Model 7\n",
      "MSE: 1545.28783028\n",
      "R2: 0.31191631575\n",
      "Model 8\n",
      "MSE: 1833.61051986\n",
      "R2: 0.311888925687\n",
      "Model 9\n",
      "MSE: 1628.09411951\n",
      "R2: 0.311926400386\n",
      "Model 10\n",
      "MSE: 1668.83279763\n",
      "R2: 0.311914489696\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.41697088\n",
      "Mean of R2 for all folds: 0.311916505954\n"
     ]
    }
   ],
   "source": [
    "kf10 = cross_validation.KFold(len(modeldata), n_folds=10, shuffle=True,random_state=1)\n",
    "from sklearn import linear_model\n",
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf10:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "Model 1\n",
      "MSE: 1632.6576737\n",
      "R2: 0.311930135282\n",
      "Model 2\n",
      "MSE: 1846.31180986\n",
      "R2: 0.311930899613\n",
      "Model 3\n",
      "MSE: 1669.50563349\n",
      "R2: 0.311931128731\n",
      "Model 4\n",
      "MSE: 1808.15345738\n",
      "R2: 0.311929540908\n",
      "Model 5\n",
      "MSE: 2337.49328167\n",
      "R2: 0.311927759009\n",
      "Model 6\n",
      "MSE: 1870.07361735\n",
      "R2: 0.311933003088\n",
      "Model 7\n",
      "MSE: 1473.38838172\n",
      "R2: 0.311927942138\n",
      "Model 8\n",
      "MSE: 1778.23126457\n",
      "R2: 0.311929304945\n",
      "Model 9\n",
      "MSE: 1150.24104632\n",
      "R2: 0.311930155711\n",
      "Model 10\n",
      "MSE: 1628.37388847\n",
      "R2: 0.311930814742\n",
      "Model 11\n",
      "MSE: 1554.65371038\n",
      "R2: 0.311932756302\n",
      "Model 12\n",
      "MSE: 1355.77554584\n",
      "R2: 0.311926280294\n",
      "Model 13\n",
      "MSE: 1793.0613788\n",
      "R2: 0.311932467335\n",
      "Model 14\n",
      "MSE: 1585.03980882\n",
      "R2: 0.311933731228\n",
      "Model 15\n",
      "MSE: 1737.53024958\n",
      "R2: 0.311932443226\n",
      "Model 16\n",
      "MSE: 1726.1876977\n",
      "R2: 0.311930351066\n",
      "Model 17\n",
      "MSE: 1857.19303881\n",
      "R2: 0.311928433741\n",
      "Model 18\n",
      "MSE: 1945.39751044\n",
      "R2: 0.311928194612\n",
      "Model 19\n",
      "MSE: 1309.68058113\n",
      "R2: 0.311922466671\n",
      "Model 20\n",
      "MSE: 1570.6798169\n",
      "R2: 0.311931142063\n",
      "Model 21\n",
      "MSE: 1850.67656141\n",
      "R2: 0.311929402346\n",
      "Model 22\n",
      "MSE: 1566.60055198\n",
      "R2: 0.311931775442\n",
      "Model 23\n",
      "MSE: 1813.00917444\n",
      "R2: 0.311933984221\n",
      "Model 24\n",
      "MSE: 1807.24001526\n",
      "R2: 0.311933757412\n",
      "Model 25\n",
      "MSE: 1548.87107779\n",
      "R2: 0.311928253345\n",
      "Model 26\n",
      "MSE: 1921.89508953\n",
      "R2: 0.311931876937\n",
      "Model 27\n",
      "MSE: 1874.007839\n",
      "R2: 0.311933788783\n",
      "Model 28\n",
      "MSE: 1472.09155912\n",
      "R2: 0.311931421876\n",
      "Model 29\n",
      "MSE: 1294.05939611\n",
      "R2: 0.311932729116\n",
      "Model 30\n",
      "MSE: 1689.34046012\n",
      "R2: 0.31193315487\n",
      "Model 31\n",
      "MSE: 1778.29878356\n",
      "R2: 0.311925058487\n",
      "Model 32\n",
      "MSE: 1203.91866695\n",
      "R2: 0.31193260343\n",
      "Model 33\n",
      "MSE: 1474.42962025\n",
      "R2: 0.311932236399\n",
      "Model 34\n",
      "MSE: 1354.76296552\n",
      "R2: 0.311930422487\n",
      "Model 35\n",
      "MSE: 1758.01003546\n",
      "R2: 0.311931214373\n",
      "Model 36\n",
      "MSE: 1845.14879683\n",
      "R2: 0.311927633879\n",
      "Model 37\n",
      "MSE: 1618.36876974\n",
      "R2: 0.311933785474\n",
      "Model 38\n",
      "MSE: 1956.71962515\n",
      "R2: 0.311930797022\n",
      "Model 39\n",
      "MSE: 2041.58441517\n",
      "R2: 0.311932767358\n",
      "Model 40\n",
      "MSE: 1671.39510075\n",
      "R2: 0.311932777721\n",
      "Model 41\n",
      "MSE: 1825.88797379\n",
      "R2: 0.311931979809\n",
      "Model 42\n",
      "MSE: 1765.80204981\n",
      "R2: 0.311931102629\n",
      "Model 43\n",
      "MSE: 1659.79386545\n",
      "R2: 0.311931602067\n",
      "Model 44\n",
      "MSE: 1597.72193495\n",
      "R2: 0.311933101154\n",
      "Model 45\n",
      "MSE: 1305.9639072\n",
      "R2: 0.311933092803\n",
      "Model 46\n",
      "MSE: 1164.20987504\n",
      "R2: 0.311929316632\n",
      "Model 47\n",
      "MSE: 1718.63628302\n",
      "R2: 0.311931424011\n",
      "Model 48\n",
      "MSE: 1540.94272729\n",
      "R2: 0.311933992482\n",
      "Model 49\n",
      "MSE: 2347.96629828\n",
      "R2: 0.311922844753\n",
      "Model 50\n",
      "MSE: 1577.66053385\n",
      "R2: 0.311931013346\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.49286692\n",
      "Mean of R2 for all folds: 0.311930757227\n"
     ]
    }
   ],
   "source": [
    "kf50 = cross_validation.KFold(len(modeldata), n_folds=50, shuffle=True,random_state=1)\n",
    "from sklearn import linear_model\n",
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf50:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Experiment with differen model evaluation metrics form sci-kit learn\n",
    "\n",
    "See a list of model evaluatin metrics here. Particulary try precision and recall and f1. Read about available methods and what they mean here:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58119658119658124"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " metrics.f1_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53773585,  0.58119658])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " metrics.f1_score(y_test, y_pred_class, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55946621512659256"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " metrics.f1_score(y_test, y_pred_class, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5605381165919282"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred_class, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55625051073058518"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred_class, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74564459930313576"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcualte F1 from confusing matrix directly\n",
    "F1=2*cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[1,0])*cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1])/(cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[1,0])+cnf_mat[0,0]/float(cnf_mat[0,0]+cnf_mat[0,1]))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
