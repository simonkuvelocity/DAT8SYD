{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 3 - Logistic Regression\n",
    "\n",
    "## EXERCISE: Predicting Survival on the Titanic\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 1: read the data from titanic.csv into a DataFrame\n",
    "import pandas as pd\n",
    "titanic = pd.read_csv('../../data/titanic.csv', index_col='PassengerId')\n",
    "print(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TASK 2: define Pclass/Parch as the features and Survived as the response\n",
    "feature_cols = ['Pclass', 'Parch']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 3: split the data into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 4: fit a logistic regression model and examine the coefficients\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "print(logreg.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 5: make predictions on testing set and calculate accuracy\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 6: add Age as a feature and calculate testing accuracy\n",
    "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n",
    "feature_cols = ['Pclass', 'Parch', 'Age']\n",
    "X = titanic[feature_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 7 : Confusion Matrix\n",
    "from sklearn import metrics\n",
    "prds = logreg.predict(X)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK 8: Generate the ROC Curve\n",
    "\n",
    "# Import the Plotting library so we can draw the graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the prediction values for each of the test observations using predict_proba() function rather than just predict\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Store the false positive rate(fpr), true positive rate (tpr) in vectors for use in the graph\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "\n",
    "# Store the Area Under the Curve (AUC) so we can annotate our graph with theis metric\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# What's happening here is we are changing the cutoff value from 0 to 1.\n",
    "# When we have a cutoff of zero this means that we have no positive predictions so both fpr and tpr are both 0\n",
    "# Our aim when modelling is to maximise the area under the curve, the closer to one the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TASK 9: What variables should we include in the model to improve it?\n",
    "\n",
    "# Firstly, let's convert our character vectors to binary represnetations so we can use them in the model\n",
    "titanic_with_dummies = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass'], prefix = ['Sex', 'Embarked', 'Pclass'] )\n",
    "\n",
    "# Inspect the new data set with one-hot encoding done through pandas\n",
    "#print(titanic_with_dummies.head())\n",
    "\n",
    "# For now we will just fill the age column by taking the average by sex, class and Parch\n",
    "# Later on we will use methods that can handle missing values better.\n",
    "titanic_with_dummies['Age'] = titanic_with_dummies[['Age',\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2']].groupby([\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "# Now let's include them one by one and see if they improve the model\n",
    "# NOTE!!! We don't do this in practive and will learn better techniques mext week. But the idea is useful to thing about\n",
    "feature_cols = ['Parch', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Fare', 'Age']\n",
    "X = titanic_with_dummies[feature_cols]\n",
    "y = titanic_with_dummies.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the ROC curve for our ne model\n",
    "\n",
    "# Generate the prediction values for each of the test observations using predict_proba() function rather than just predict\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Store the false positive rate(fpr), true positive rate (tpr) in vectors for use in the graph\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "\n",
    "# Store the Area Under the Curve (AUC) so we can annotate our graph with theis metric\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Notice how the area under the curve changes when we imporve the model.\n",
    "\n",
    "# Actually, we can fit an even better model than this, try and beat this model \n",
    "# Prize for the first person to get accuracy above 0.85 on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 10: Try a different classification algorithm like Naive Bayes or Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_model = KNeighborsClassifier(5)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred_class = KNN_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_pred_class = NB_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
